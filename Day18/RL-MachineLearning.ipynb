{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47541391-391c-4692-81ec-9ce7ac021ae9",
   "metadata": {},
   "source": [
    "# Reinforcement Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f7a3628-6fc2-4943-9341-bcc4303b6eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3463bc31-5c79-4422-add6-71181e784fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Find out steps and actions \n",
    "states = np.arange(16,31)\n",
    "actions = ['ON', 'OFF']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "879d8121-420e-4b01-863b-2a25e91e1984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16 17 18 19 20 21 22 23 24 25 26 27 28 29 30]\n",
      "['ON', 'OFF']\n"
     ]
    }
   ],
   "source": [
    "print (states) \n",
    "print (actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "549021cd-b4d5-47c8-8bbc-6b59cae14b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Create Qtable set alpha, gama, epsilon, episodes\n",
    "# Q table is [s,a] -- s: state, a:action\n",
    "Q = np.zeros((len(states),len(actions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b13d9900-6d3d-440c-bc8e-3d8c674fec66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#alpha: (0-1) -- range of trust?\n",
    "#gamma: (0-1) \n",
    "#epsilon -- probability of exploring new actions for current best \n",
    "#episodes -- time that you want to give \n",
    "alpha = 0.1 \n",
    "gamma = 0.9 \n",
    "epsilon = 0.2\n",
    "episodes = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fdcd8a69-d308-4df3-a6ad-409a067a6e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step3: Write Reward function \n",
    "def get_reward(temp,action): \n",
    "    if 20 <= temp <= 24: \n",
    "        reward = 10 \n",
    "    else: \n",
    "        reward = -5\n",
    "    if action == 'ON': \n",
    "        reward -= 2 \n",
    "    return reward \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a390c126-d284-4c7d-bd30-af35896b3a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step4: Environment Dynamic Change (e.g. Temperature Changes) \n",
    "def next_temp(temp, action):\n",
    "    if action == 'ON': \n",
    "        temp -= random.choice([1,2])\n",
    "    else: \n",
    "        temp+= random.choice([0,1,2])\n",
    "    return int (np.clip(temp,16,30))\n",
    "#clip lets value remain between the range defined - 30+ -> 30, 16- -> 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d0feccd5-31b5-4149-ba2f-aee2d8c04a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Done\n"
     ]
    }
   ],
   "source": [
    "#Step 5 Training Loop \n",
    "for ep in range(episodes): \n",
    "    temp = random.choice(states) \n",
    "    done = False \n",
    "    for i in range(20): #Limit steps per episode\n",
    "        #choose action (epsilon-greedy) \n",
    "        if random.uniform(0,1) < epsilon: \n",
    "            action = random.choice(actions) \n",
    "        else: \n",
    "            action = actions[np.argmax(Q[temp - 16])]\n",
    "\n",
    "        next_state = next_temp(temp,action)\n",
    "        reward = get_reward(next_state,action)\n",
    "        a = actions.index(action)\n",
    "        best_next=np.max(Q[next_state - 16])\n",
    "        Q[temp - 16, a] += alpha * (reward + gamma * best_next - Q[temp-16, a])\n",
    "\n",
    "print (\"Training Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2387497a-174b-4530-884b-51a8a9238c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Temp=18C -> Action=OFF\n",
      "Step 2: Temp=18C -> Action=OFF\n",
      "Step 3: Temp=20C -> Action=OFF\n",
      "Step 4: Temp=20C -> Action=OFF\n",
      "Step 5: Temp=22C -> Action=ON\n",
      "Step 6: Temp=21C -> Action=OFF\n",
      "Step 7: Temp=21C -> Action=OFF\n",
      "Step 8: Temp=21C -> Action=OFF\n",
      "Step 9: Temp=21C -> Action=OFF\n",
      "Step 10: Temp=21C -> Action=OFF\n"
     ]
    }
   ],
   "source": [
    "# Test Learned policy \n",
    "temp = 18\n",
    "for step in range(10): \n",
    "    action = actions[np.argmax(Q[temp - 16])]\n",
    "    print (f\"Step {step+1}: Temp={temp}C -> Action={action}\")\n",
    "    temp=next_temp(temp,action) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6a7ee833-6d0d-4304-9f11-dd8082d538f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter starting toom temp(16-30) 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temp out of range\n",
      "Set to default 25C\n",
      "Starting temp: 25 C\n",
      "Step 1: Temp=25C --> ActionON\n",
      "Step 2: Temp=24C --> ActionON\n",
      "Step 3: Temp=23C --> ActionON\n",
      "Step 4: Temp=21C --> ActionOFF\n",
      "Step 5: Temp=21C --> ActionOFF\n",
      "Step 6: Temp=23C --> ActionON\n",
      "Step 7: Temp=22C --> ActionON\n",
      "Step 8: Temp=21C --> ActionOFF\n",
      "Step 9: Temp=23C --> ActionON\n",
      "Step 10: Temp=22C --> ActionON\n",
      "\n",
      " Done\n"
     ]
    }
   ],
   "source": [
    "#Use user input \n",
    "try: \n",
    "    temp=int(input('Enter starting toom temp(16-30)'))\n",
    "    if temp<16 or temp>30: \n",
    "             raise ValueError('Temp out of range')\n",
    "except ValueError as ve: \n",
    "    print (ve) \n",
    "    temp=25\n",
    "    print ('Set to default 25C')\n",
    "print(f'Starting temp: {temp} C')\n",
    "\n",
    "for step in range(10): \n",
    "    action = actions[np.argmax(Q[temp -16])]\n",
    "    print(f\"Step {step+1}: Temp={temp}C --> Action{action}\")\n",
    "    temp = next_temp(temp, action)\n",
    "print (\"\\n Done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b114bc-e5ef-4d23-a1ac-4bd4106ee553",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
