{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V5E1"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##We are going to discuss NP Basics here.\n",
        "NLP Pipeline"
      ],
      "metadata": {
        "id": "rtkPXeLHeaiX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import required libraries\n",
        "import nltk #Python library - natural language tool kit\n",
        "import spacy\n",
        "from nltk.corpus import stopwords #corpus - collection of words\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer #remove suffix, prefix\n",
        "import re\n",
        "#for vectorization\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "IVReQVyQeZq0"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4') #multilingual"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ck0GBx09fIyX",
        "outputId": "d83afffb-a249-48da-ba31-30b533dfef63"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "WvMsIFK_eRqG"
      },
      "outputs": [],
      "source": [
        "our_text=\"Artificial Intelligence and Machine Learning are going to change @the future of Technology!\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens=word_tokenize(our_text)\n",
        "#splits text into individual words/tokens\n",
        "tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bP-F-PqofFwZ",
        "outputId": "d103de9a-2d01-4fcf-aec0-a62b6cc14a52"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Artificial',\n",
              " 'Intelligence',\n",
              " 'and',\n",
              " 'Machine',\n",
              " 'Learning',\n",
              " 'are',\n",
              " 'going',\n",
              " 'to',\n",
              " 'change',\n",
              " '@',\n",
              " 'the',\n",
              " 'future',\n",
              " 'of',\n",
              " 'Technology',\n",
              " '!']"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clean_tokens=[re.sub(r'[^\\w\\s]','',token.lower()) for token in tokens if token.isalpha()]\n",
        "#tokenisalpha() filter to check for all characters is alphabets --> returns True/False\n",
        "clean_tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R-G1MZuBi8sb",
        "outputId": "4683821e-7af3-47db-b451-8f645a9fcaec"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['artificial',\n",
              " 'intelligence',\n",
              " 'and',\n",
              " 'machine',\n",
              " 'learning',\n",
              " 'are',\n",
              " 'going',\n",
              " 'to',\n",
              " 'change',\n",
              " 'the',\n",
              " 'future',\n",
              " 'of',\n",
              " 'technology']"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words=set(stopwords.words('english'))\n",
        "filtered_tokens=[token for token in clean_tokens if token not in stop_words]\n",
        "#removes common less meaningful words - and, are, to, of\n",
        "filtered_tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6RgwE5rCjcse",
        "outputId": "9098ab55-ff73-4264-f22d-a227dd068938"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['artificial',\n",
              " 'intelligence',\n",
              " 'machine',\n",
              " 'learning',\n",
              " 'going',\n",
              " 'change',\n",
              " 'future',\n",
              " 'technology']"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#stemming - back to root\n",
        "stemmer = PorterStemmer()\n",
        "stemmed_tokens=[stemmer.stem(token) for token in filtered_tokens]\n",
        "stemmed_tokens\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AOMHFf8rk9wD",
        "outputId": "045c139e-ffaf-4f04-b13c-1fb719056aec"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['artifici',\n",
              " 'intellig',\n",
              " 'machin',\n",
              " 'learn',\n",
              " 'go',\n",
              " 'chang',\n",
              " 'futur',\n",
              " 'technolog']"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Lemmatization - convert words to their dictionary form (lemma) using POS (part of speech)\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_tokens=[lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
        "lemmatized_tokens\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eRs1sOAZljLC",
        "outputId": "14afbb1f-a49e-47ff-a4d5-ce45269958e3"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['artificial',\n",
              " 'intelligence',\n",
              " 'machine',\n",
              " 'learning',\n",
              " 'going',\n",
              " 'change',\n",
              " 'future',\n",
              " 'technology']"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bag of Words (BOW)"
      ],
      "metadata": {
        "id": "j1K-dugonj6y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Vectorization\n",
        "from nltk import corpus\n",
        "corpus = [\n",
        "    \"Artificial Intelligence is transforming Industries\",\n",
        "    \"Machine Learning is a branch of artificial intelligence\",\n",
        "    \"Deep Learning is a subfield of machine learning\",\n",
        "    \"Natural Language Processing is a subfield of artificial intelligence\"\n",
        "]"
      ],
      "metadata": {
        "id": "sif66eYAmVdW"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv=CountVectorizer()\n",
        "bow_matrix=cv.fit_transform(corpus)\n",
        "print(\"--- Features Names (BOW) ---\")\n",
        "print(cv.get_feature_names_out())\n",
        "print(\"--- BOW Matrix ---\")\n",
        "print(bow_matrix.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T-Knk8VfoTrL",
        "outputId": "29d256d6-2e89-4111-dea4-cd1ee86954bf"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Features Names (BOW) ---\n",
            "['artificial' 'branch' 'deep' 'industries' 'intelligence' 'is' 'language'\n",
            " 'learning' 'machine' 'natural' 'of' 'processing' 'subfield'\n",
            " 'transforming']\n",
            "--- BOW Matrix ---\n",
            "[[1 0 0 1 1 1 0 0 0 0 0 0 0 1]\n",
            " [1 1 0 0 1 1 0 1 1 0 1 0 0 0]\n",
            " [0 0 1 0 0 1 0 2 1 0 1 0 1 0]\n",
            " [1 0 0 0 1 1 1 0 0 1 1 1 1 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LAgI0-CeopES"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}